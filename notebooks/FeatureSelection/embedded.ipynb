{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac71498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef4f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\User\\Desktop\\ML_Lesson\\Projects\\Project3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9547bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c85a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12377 entries, 0 to 12376\n",
      "Data columns (total 28 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Model                        12377 non-null  float64\n",
      " 1   Year_of_Manufacture          12377 non-null  float64\n",
      " 2   Number_of_Engines            12377 non-null  float64\n",
      " 3   Capacity                     12377 non-null  float64\n",
      " 4   Range_(km)                   12377 non-null  int64  \n",
      " 5   Fuel_Consumption_(L/hour)    12377 non-null  float64\n",
      " 6   Hourly_Maintenance_Cost_($)  12377 non-null  float64\n",
      " 7   Age                          12377 non-null  float64\n",
      " 8   Sales_Region                 12377 non-null  float64\n",
      " 9   Price_($)                    12377 non-null  float64\n",
      " 10  HMC_per_person               12377 non-null  float64\n",
      " 11  Engine_Power_Factor          12377 non-null  float64\n",
      " 12  Price_per_Seat               12377 non-null  float64\n",
      " 13  Seats_per_Engine             12377 non-null  float64\n",
      " 14  Company_Popularity           12377 non-null  float64\n",
      " 15  FuelCost_Maint_Index         12377 non-null  float64\n",
      " 16  Engine_to_Capacity           12377 non-null  float64\n",
      " 17  Engine_Type_Piston           12377 non-null  float64\n",
      " 18  Engine_Type_Turbofan         12377 non-null  float64\n",
      " 19  Company_Airbus               12377 non-null  float64\n",
      " 20  Company_Boeing               12377 non-null  float64\n",
      " 21  Company_Bombardier           12377 non-null  float64\n",
      " 22  Company_Cessna               12377 non-null  float64\n",
      " 23  Age_Group_0-10               12377 non-null  float64\n",
      " 24  Age_Group_10-20              12377 non-null  float64\n",
      " 25  Age_Group_20-30              12377 non-null  float64\n",
      " 26  Age_Group_30-40              12377 non-null  float64\n",
      " 27  Age_Group_40+                12377 non-null  float64\n",
      "dtypes: float64(27), int64(1)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b5364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12377 entries, 0 to 12376\n",
      "Data columns (total 28 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Model                        12377 non-null  float64\n",
      " 1   Year_of_Manufacture          12377 non-null  float64\n",
      " 2   Number_of_Engines            12377 non-null  float64\n",
      " 3   Capacity                     12377 non-null  float64\n",
      " 4   Range_(km)                   12377 non-null  int64  \n",
      " 5   Fuel_Consumption_(L/hour)    12377 non-null  float64\n",
      " 6   Hourly_Maintenance_Cost_($)  12377 non-null  float64\n",
      " 7   Age                          12377 non-null  float64\n",
      " 8   Sales_Region                 12377 non-null  float64\n",
      " 9   Price_($)                    12377 non-null  float64\n",
      " 10  HMC_per_person               12377 non-null  float64\n",
      " 11  Engine_Power_Factor          12377 non-null  float64\n",
      " 12  Price_per_Seat               12377 non-null  float64\n",
      " 13  Seats_per_Engine             12377 non-null  float64\n",
      " 14  Company_Popularity           12377 non-null  float64\n",
      " 15  FuelCost_Maint_Index         12377 non-null  float64\n",
      " 16  Engine_to_Capacity           12377 non-null  float64\n",
      " 17  Engine_Type_Piston           12377 non-null  float64\n",
      " 18  Engine_Type_Turbofan         12377 non-null  float64\n",
      " 19  Company_Airbus               12377 non-null  float64\n",
      " 20  Company_Boeing               12377 non-null  float64\n",
      " 21  Company_Bombardier           12377 non-null  float64\n",
      " 22  Company_Cessna               12377 non-null  float64\n",
      " 23  Age_Group_0-10               12377 non-null  float64\n",
      " 24  Age_Group_10-20              12377 non-null  float64\n",
      " 25  Age_Group_20-30              12377 non-null  float64\n",
      " 26  Age_Group_30-40              12377 non-null  float64\n",
      " 27  Age_Group_40+                12377 non-null  float64\n",
      "dtypes: float64(27), int64(1)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('Range_(km)', axis=1)   \n",
    "y = df['Range_(km)'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10248576",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911e6a2",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8ccc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2: 1.0\n",
      "Linear Regression MAE: 1.2296985472614166e-10\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "lr_score = r2_score(y_test, y_pred)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred)\n",
    "lr_scores = cross_val_score(lr, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Linear Regression R2:\", lr_score)\n",
    "print(\"Linear Regression MAE:\", lr_mae)\n",
    "print(\"K-Fold mean:\", lr_scores.mean())\n",
    "print(\"K-Fold std:\", lr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ede96",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356e0c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.479e+07, tolerance: 2.982e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+08, tolerance: 2.982e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.560e+07, tolerance: 2.498e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.584e+07, tolerance: 2.481e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.665e+07, tolerance: 2.488e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression R2: 0.9989882019526748\n",
      "Lasso Regression MAE: 146.53699302339467\n",
      "K-Fold mean: 0.9989843496244792\n",
      "K-Fold std: 1.6861259142413586e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = x_train.columns[importance > 0]\n",
    "\n",
    "lasso.fit(x_train[selected_features], y_train)\n",
    "y_pred = lasso.predict(x_test[selected_features])\n",
    "\n",
    "lasso_score = r2_score(y_test, y_pred)\n",
    "lasso_mae = mean_absolute_error(y_test, y_pred)\n",
    "lasso_scores = cross_val_score(lasso, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Lasso Regression R2:\", lasso_score)\n",
    "print(\"Lasso Regression MAE:\", lasso_mae)\n",
    "print(\"K-Fold mean:\", lasso_scores.mean())\n",
    "print(\"K-Fold std:\", lasso_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e6859",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927200e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2: 0.9995001033591128\n",
      "Ridge Regression MAE: 104.33644948617068\n",
      "K-Fold mean: 0.9994050774605684\n",
      "K-Fold std: 8.876316164482515e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "importance = np.abs(ridge.coef_)\n",
    "selected_features = x_train.columns[importance > 0]\n",
    "\n",
    "ridge.fit(x_train[selected_features], y_train)\n",
    "y_pred = ridge.predict(x_test[selected_features])\n",
    "\n",
    "ridge_score = r2_score(y_test, y_pred)\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred)\n",
    "ridge_scores = cross_val_score(ridge, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Ridge Regression R2:\", ridge_score)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae)\n",
    "print(\"K-Fold mean:\", ridge_scores.mean())\n",
    "print(\"K-Fold std:\", ridge_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaf00a",
   "metadata": {},
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd50422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNetn R2: 0.7219430655706736\n",
      "ElasticNet MAE: 2694.7125118771805\n",
      "K-Fold mean: 0.7170114070935142\n",
      "K-Fold std: 0.0008568925122605994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = ElasticNet(random_state=42)\n",
    "elastic.fit(x_train, y_train)\n",
    "\n",
    "importance = np.abs(elastic.coef_)\n",
    "selected_features = x_train.columns[importance > 0]\n",
    "\n",
    "elastic.fit(x_train[selected_features], y_train)\n",
    "y_pred = elastic.predict(x_test[selected_features])\n",
    "\n",
    "elastic_score = r2_score(y_test, y_pred)\n",
    "elastic_mae = mean_absolute_error(y_test, y_pred)\n",
    "elastic_scores = cross_val_score(elastic, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"ElasticNetn R2:\", elastic_score)\n",
    "print(\"ElasticNet MAE:\", elastic_mae)\n",
    "print(\"K-Fold mean:\", elastic_scores.mean())\n",
    "print(\"K-Fold std:\", elastic_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0b2b6",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8340fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor R2: 1.0\n",
      "Decision Tree Regressor MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test[selected_features])\n",
    "\n",
    "dt_score = r2_score(y_test, y_pred)\n",
    "dt_mae = mean_absolute_error(y_test, y_pred)\n",
    "dt_scores = cross_val_score(dt, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Decision Tree Regressor R2:\", dt_score)\n",
    "print(\"Decision Tree Regressor MAE:\", dt_mae)\n",
    "print(\"K-Fold mean:\", dt_scores.mean())\n",
    "print(\"K-Fold std:\", dt_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f97dd",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e3a9296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor R2: 1.0\n",
      "Random Forest Regressor MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "importance = rf.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "rf.fit(x_train[selected_features], y_train)\n",
    "y_pred = rf.predict(x_test[selected_features])\n",
    "\n",
    "rf_score = r2_score(y_test, y_pred)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred)\n",
    "rf_scores = cross_val_score(rf, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Random Forest Regressor R2:\", rf_score)\n",
    "print(\"Random Forest Regressor MAE:\", rf_mae)\n",
    "print(\"K-Fold mean:\", rf_scores.mean())\n",
    "print(\"K-Fold std:\", rf_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4690ab",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f964a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor R2: 0.9999999992943458\n",
      "Gradient Boosting Regressor MAE: 0.1318368600822984\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(x_train, y_train)\n",
    "\n",
    "importance = gbr.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "gbr.fit(x_train[selected_features], y_train)\n",
    "y_pred = gbr.predict(x_test[selected_features])\n",
    "\n",
    "gbr_score = r2_score(y_test, y_pred)\n",
    "gbr_mae = mean_absolute_error(y_test, y_pred)\n",
    "gbr_scores = cross_val_score(gbr, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Gradient Boosting Regressor R2:\", gbr_score)\n",
    "print(\"Gradient Boosting Regressor MAE:\", gbr_mae)\n",
    "print(\"K-Fold mean:\", gbr_scores.mean())\n",
    "print(\"K-Fold std:\", gbr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332a05",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d14fc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Regressor R2: 1.0\n",
      "Extra Tree Regressor MAE: 0.0\n",
      "K-Fold mean: 0.9999999980329912\n",
      "K-Fold std: 2.781770595941884e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=42)\n",
    "et.fit(x_train, y_train)\n",
    "\n",
    "importance = et.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "et.fit(x_train[selected_features], y_train)\n",
    "y_pred = et.predict(x_test[selected_features])\n",
    "\n",
    "et_score = r2_score(y_test, y_pred)\n",
    "et_mae = mean_absolute_error(y_test, y_pred)\n",
    "et_scores = cross_val_score(et, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Extra Tree Regressor R2:\", et_score)\n",
    "print(\"Extra Tree Regressor MAE:\", et_mae)\n",
    "print(\"K-Fold mean:\", et_scores.mean())\n",
    "print(\"K-Fold std:\", et_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feda23c",
   "metadata": {},
   "source": [
    "# Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "286b5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting Regressor R2: 0.9999999992943458\n",
      "Hist Gradient Boosting Regressor MAE: 0.1318368607055448\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "hgb.fit(x_train, y_train)\n",
    "y_pred = hgb.predict(x_test)\n",
    "\n",
    "hgb_score = r2_score(y_test, y_pred)\n",
    "hgb_mae = mean_absolute_error(y_test, y_pred)\n",
    "hgb_scores = cross_val_score(hgb, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Hist Gradient Boosting Regressor R2:\", hgb_score)\n",
    "print(\"Hist Gradient Boosting Regressor MAE:\", hgb_mae)\n",
    "print(\"K-Fold mean:\", hgb_scores.mean())\n",
    "print(\"K-Fold std:\", hgb_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5218a",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a92b2196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR R2: 0.9893018073436654\n",
      "SVR MAE: 342.71961704091615\n",
      "K-Fold mean: 0.978837127553549\n",
      "K-Fold std: 0.0008034739843721698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='rbf', C=20.0)\n",
    "\n",
    "svr.fit(x_train, y_train)\n",
    "y_pred = svr.predict(x_test)\n",
    "\n",
    "svr_score = r2_score(y_test, y_pred)\n",
    "svr_mae = mean_absolute_error(y_test, y_pred)\n",
    "svr_scores = cross_val_score(svr, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"SVR R2:\", svr_score)\n",
    "print(\"SVR MAE:\", svr_mae)\n",
    "print(\"K-Fold mean:\", svr_scores.mean())\n",
    "print(\"K-Fold std:\", svr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076f8b2",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8329f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN R2: 1.0\n",
      "KNN MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "knn_score = r2_score(y_test, y_pred)\n",
    "knn_mae = mean_absolute_error(y_test, y_pred)\n",
    "knn_scores = cross_val_score(knn, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"KNN R2:\", knn_score)\n",
    "print(\"KNN MAE:\", knn_mae)\n",
    "print(\"K-Fold mean:\", knn_scores.mean())\n",
    "print(\"K-Fold std:\", knn_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70194d8d",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3268a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor R2: 1.0\n",
      "XGBoost Regressor MAE: 0.0005456188227981329\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "importance = xgb.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "xgb.fit(x_train[selected_features], y_train)\n",
    "y_pred = xgb.predict(x_test[selected_features])\n",
    "\n",
    "xgb_score = r2_score(y_test, y_pred)\n",
    "xgb_mae = mean_absolute_error(y_test, y_pred)\n",
    "xgb_scores = cross_val_score(xgb, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"XGBoost Regressor R2:\", xgb_score)\n",
    "print(\"XGBoost Regressor MAE:\", xgb_mae)\n",
    "print(\"K-Fold mean:\", xgb_scores.mean())\n",
    "print(\"K-Fold std:\", xgb_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9577b",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6756a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Regressor R2: 1.0\n",
      "Adaboost Regressor MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ab = AdaBoostRegressor(random_state=42)\n",
    "ab.fit(x_train, y_train)\n",
    "\n",
    "importance = ab.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "ab.fit(x_train[selected_features], y_train)\n",
    "y_pred = ab.predict(x_test[selected_features])\n",
    "\n",
    "ab_score = r2_score(y_test, y_pred)\n",
    "ab_mae = mean_absolute_error(y_test, y_pred)\n",
    "ab_scores = cross_val_score(ab, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Adaboost Regressor R2:\", ab_score)\n",
    "print(\"Adaboost Regressor MAE:\", ab_mae)\n",
    "print(\"K-Fold mean:\", ab_scores.mean())\n",
    "print(\"K-Fold std:\", ab_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb499de7",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "585596a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1678\n",
      "[LightGBM] [Info] Number of data points in the train set: 9901, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 7798.079992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 566\n",
      "[LightGBM] [Info] Number of data points in the train set: 9901, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 7798.079992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 566\n",
      "[LightGBM] [Info] Number of data points in the train set: 8251, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 7792.443340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 566\n",
      "[LightGBM] [Info] Number of data points in the train set: 8251, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 7760.310265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 566\n",
      "[LightGBM] [Info] Number of data points in the train set: 8252, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 7793.875424\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LGBM Regressor R2: 0.9999999992943458\n",
      "LGBM Regressor MAE: 0.1318368607055448\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "lgbm.fit(x_train, y_train)\n",
    "\n",
    "importance = lgbm.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "lgbm.fit(x_train[selected_features], y_train)\n",
    "y_pred = lgbm.predict(x_test[selected_features])\n",
    "\n",
    "lgbm_score = r2_score(y_test, y_pred)\n",
    "lgbm_mae = mean_absolute_error(y_test, y_pred)\n",
    "lgbm_scores = cross_val_score(lgbm, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"LGBM Regressor R2:\", lgbm_score)\n",
    "print(\"LGBM Regressor MAE:\", lgbm_mae)\n",
    "print(\"K-Fold mean:\", lgbm_scores.mean())\n",
    "print(\"K-Fold std:\", lgbm_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adfbcd",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52257cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regression R2: 1.0\n",
      "Bagging Regression MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor()\n",
    "\n",
    "bag.fit(x_train, y_train)\n",
    "y_pred = bag.predict(x_test)\n",
    "\n",
    "bag_score = r2_score(y_test, y_pred)\n",
    "bag_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_scores = cross_val_score(bag, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagging Regression R2:\", bag_score)\n",
    "print(\"Bagging Regression MAE:\", bag_mae)\n",
    "print(\"K-Fold mean:\", bag_scores.mean())\n",
    "print(\"K-Fold std:\", bag_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abbd3d",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c0a65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VotingRegressor.__init__() got an unexpected keyword argument 'voting'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m model2 = ExtraTreesRegressor(n_estimators=\u001b[32m200\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     12\u001b[39m model3 = LinearRegression()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m voting_hard = \u001b[43mVotingRegressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43met\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoting\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhard\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m voting_hard.fit(x_train[selected_features], y_train)\n\u001b[32m     24\u001b[39m y_pred = voting_hard.predict(x_test[selected_features])\n",
      "\u001b[31mTypeError\u001b[39m: VotingRegressor.__init__() got an unexpected keyword argument 'voting'"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# rf_fs.fit(x_train, y_train)\n",
    "\n",
    "# importance = rf_fs.feature_importances_\n",
    "# selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "# model1 = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "# model2 = ExtraTreesRegressor(n_estimators=200, random_state=42)\n",
    "# model3 = LinearRegression()\n",
    "\n",
    "# voting_hard = VotingRegressor(\n",
    "#     estimators=[\n",
    "#         ('rf', model1),\n",
    "#         ('et', model2),\n",
    "#         ('lr', model3)\n",
    "#     ],\n",
    "#     voting='hard'\n",
    "# )\n",
    "\n",
    "# voting_hard.fit(x_train[selected_features], y_train)\n",
    "# y_pred = voting_hard.predict(x_test[selected_features])\n",
    "\n",
    "# hard_voting_score = r2_score(y_test, y_pred)\n",
    "# hard_voting_mae = mean_absolute_error(y_test, y_pred)\n",
    "# hard_voting_scores = cross_val_score(voting_hard, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "# print(\"Hard Voting Regressor R2:\", hard_voting_score)\n",
    "# print(\"Hard Voting Regressor MAE:\", hard_voting_mae)\n",
    "# print(\"K-Fold mean R2:\", hard_voting_scores.mean())\n",
    "# print(\"K-Fold std R2:\", hard_voting_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d2d69",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963053d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Accuracy for type: 0.858252427184466\n",
      "Soft Voting Accuracy for cvss_score : 0.670873786407767\n",
      "K-fold F1 mean (type): 0.6141315241406103\n",
      "K-fold F1 std  (type): 0.007163728409382124\n",
      "K-fold F1 mean (cvss_score) : 0.5155132108741932\n",
      "K-fold F1 std  (cvss_score) : 0.00968546053559784\n",
      "\n",
      "Classification Report for 'type':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.04      0.07        28\n",
      "         1.0       0.98      0.80      0.88        50\n",
      "         2.0       0.80      0.36      0.50       103\n",
      "         3.0       0.74      0.81      0.78       200\n",
      "         4.0       0.81      0.97      0.88       834\n",
      "         5.0       0.96      0.54      0.69        41\n",
      "         6.0       1.00      0.06      0.11        36\n",
      "         7.0       0.87      0.79      0.83       155\n",
      "         8.0       1.00      0.99      0.99       176\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.97      0.95      0.96       423\n",
      "\n",
      "    accuracy                           0.86      2060\n",
      "   macro avg       0.83      0.57      0.61      2060\n",
      "weighted avg       0.86      0.86      0.84      2060\n",
      "\n",
      "\n",
      "Classification Report for 'cvss_score':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.21      0.31       234\n",
      "         1.0       0.61      0.50      0.55       637\n",
      "         2.0       0.88      0.33      0.48        87\n",
      "         3.0       0.69      0.90      0.78      1102\n",
      "\n",
      "    accuracy                           0.67      2060\n",
      "   macro avg       0.70      0.48      0.53      2060\n",
      "weighted avg       0.67      0.67      0.64      2060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# rf_fs.fit(x_train, y_train)\n",
    "\n",
    "# importance = np.abs(lgbm.coef_)\n",
    "# selected_features = x_train.columns[importance > 0]\n",
    "\n",
    "# model1 = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "# model2 = ExtraTreesRegressor(n_estimators=200, random_state=42)\n",
    "# model3 = LinearRegression()\n",
    "\n",
    "# voting_soft = VotingRegressor(\n",
    "#     estimators=[\n",
    "#         ('rf', model1),\n",
    "#         ('et', model2),\n",
    "#         ('lr', model3)\n",
    "#     ],\n",
    "#     voting='soft'\n",
    "# )\n",
    "\n",
    "# voting_soft.fit(x_train[selected_features], y_train)\n",
    "# y_pred = voting_soft.predict(x_test[selected_features])\n",
    "\n",
    "# soft_voting_score = r2_score(y_test, y_pred)\n",
    "# soft_voting_mae = mean_absolute_error(y_test, y_pred)\n",
    "# soft_voting_scores = cross_val_score(voting_soft, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "# print(\"Soft Voting Regressor R2:\", soft_voting_score)\n",
    "# print(\"Soft Voting Regressor MAE:\", soft_voting_mae)\n",
    "# print(\"K-Fold mean R2:\", soft_voting_scores.mean())\n",
    "# print(\"K-Fold std R2:\", soft_voting_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f42505",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1da836d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor R2: 0.999458268363128\n",
      "Voting Regressor MAE: 81.0352379925585\n",
      "K-Fold mean R2: 0.9994594414685026\n",
      "K-Fold std R2: 6.733941640672222e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "rf_fs.fit(x_train, y_train)\n",
    "\n",
    "importance = rf_fs.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "model1 = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "model2 = ExtraTreesRegressor(n_estimators=200, random_state=42)\n",
    "model3 = LinearRegression()\n",
    "\n",
    "voting = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    ")\n",
    "\n",
    "voting.fit(x_train[selected_features], y_train)\n",
    "y_pred = voting.predict(x_test[selected_features])\n",
    "\n",
    "voting_score = r2_score(y_test, y_pred)\n",
    "voting_mae = mean_absolute_error(y_test, y_pred)\n",
    "voting_scores = cross_val_score(voting, x[selected_features], y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Voting Regressor R2:\", voting_score)\n",
    "print(\"Voting Regressor MAE:\", voting_mae)\n",
    "print(\"K-Fold mean R2:\", voting_scores.mean())\n",
    "print(\"K-Fold std R2:\", voting_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49cca37",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4298d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor R2: 1.0\n",
      "Stacking Regressor MAE: 3.476677570857449e-11\n",
      "K-Fold mean R2: 0.999999998721019\n",
      "K-Fold std R2: 1.8087524787550552e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_fs.fit(x_train, y_train)\n",
    "\n",
    "importance = rf_fs.feature_importances_\n",
    "selected_features = x_train.columns[importance > np.mean(importance)]\n",
    "\n",
    "base1 = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "base2 = ExtraTreesRegressor(n_estimators=200, random_state=42)\n",
    "base3 = LinearRegression()\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', base1),\n",
    "        ('et', base2),\n",
    "        ('lr', base3)\n",
    "    ],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "stacking.fit(x_train[selected_features], y_train)\n",
    "y_pred = stacking.predict(x_test[selected_features])\n",
    "\n",
    "stacking_score = r2_score(y_test, y_pred)\n",
    "stacking_mae = mean_absolute_error(y_test, y_pred)\n",
    "stacking_scores = cross_val_score(stacking, x[selected_features], y,  cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Stacking Regressor R2:\", stacking_score)\n",
    "print(\"Stacking Regressor MAE:\", stacking_mae)\n",
    "print(\"K-Fold mean R2:\", stacking_scores.mean())\n",
    "print(\"K-Fold std R2:\", stacking_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78994ab1",
   "metadata": {},
   "source": [
    "# Bagged KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b71f3ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged KNN Regressor R2: 1.0\n",
      "Bagged KNN Regressor MAE: 0.0\n",
      "K-Fold mean R2: 1.0\n",
      "K-Fold std R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "bag_knn = BaggingRegressor(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag_knn.fit(x_train, y_train)\n",
    "y_pred = bag_knn.predict(x_test)\n",
    "\n",
    "bag_knn_score = r2_score(y_test, y_pred)\n",
    "bag_knn_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_knn_scores = cross_val_score(bag_knn, x_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagged KNN Regressor R2:\", bag_knn_score)\n",
    "print(\"Bagged KNN Regressor MAE:\", bag_knn_mae)\n",
    "print(\"K-Fold mean R2:\", bag_knn_scores.mean())\n",
    "print(\"K-Fold std R2:\", bag_knn_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f16b54",
   "metadata": {},
   "source": [
    "# Bagged DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8134634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged DT Regressor R2: 1.0\n",
      "Bagged DT Regressor MAE: 0.0\n",
      "K-Fold mean R2: 1.0\n",
      "K-Fold std R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag_dt.fit(x_train, y_train)\n",
    "y_pred = bag_dt.predict(x_test)\n",
    "\n",
    "bag_dt_score = r2_score(y_test, y_pred)\n",
    "bag_dt_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_dt_scores = cross_val_score(bag_dt, x_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagged DT Regressor R2:\", bag_dt_score)\n",
    "print(\"Bagged DT Regressor MAE:\", bag_dt_mae)\n",
    "print(\"K-Fold mean R2:\", bag_dt_scores.mean())\n",
    "print(\"K-Fold std R2:\", bag_dt_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36609e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Embedded Method Comparison                              </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm              </span><span style=\"font-weight: bold\"> R2 score </span><span style=\"font-weight: bold\"> Mean Absolute Error </span><span style=\"font-weight: bold\"> K-Fold mean </span><span style=\"font-weight: bold\"> K-Fold std </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Linear Regression</span>       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       \n",
       "\n",
       " Decision Tree           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Ridge                   1.00      104.34               1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      81.04                1.00         0.00       \n",
       "\n",
       " Lasso                   1.00      146.54               1.00         0.00       \n",
       "\n",
       " SVR                     0.99      342.72               0.98         0.00       \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ElasticNet</span>              <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.72</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2694.71</span>              <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.72</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Embedded Method Comparison                              \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mR2 score\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mMean Absolute Error\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mLinear Regression\u001b[0m       \u001b[1;32m1.00\u001b[0m      \u001b[1;32m0.00\u001b[0m                 \u001b[1;32m1.00\u001b[0m         \u001b[1;32m0.00\u001b[0m       \n",
       "\n",
       " Decision Tree           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Ridge                   1.00      104.34               1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      81.04                1.00         0.00       \n",
       "\n",
       " Lasso                   1.00      146.54               1.00         0.00       \n",
       "\n",
       " SVR                     0.99      342.72               0.98         0.00       \n",
       "\n",
       " \u001b[1;31mElasticNet\u001b[0m              \u001b[1;31m0.72\u001b[0m      \u001b[1;31m2694.71\u001b[0m              \u001b[1;31m0.72\u001b[0m         \u001b[1;31m0.00\u001b[0m       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "results = [\n",
    "    ['Linear Regression', lr_score, lr_mae, lr_scores.mean(), lr_scores.std()],\n",
    "    ['Lasso', lasso_score, lasso_mae, lasso_scores.mean(), lasso_scores.std()],\n",
    "    ['Ridge', ridge_score, ridge_mae, ridge_scores.mean(), ridge_scores.std()],\n",
    "    ['ElasticNet', elastic_score, elastic_mae, elastic_scores.mean(), elastic_scores.std()],\n",
    "    ['Decision Tree', dt_score, dt_mae, dt_scores.mean(), dt_scores.std()],\n",
    "    ['Random Forest', rf_score, rf_mae, rf_scores.mean(), rf_scores.std()],\n",
    "    ['Gradient Boosting', gbr_score, gbr_mae, gbr_scores.mean(), gbr_scores.std()],\n",
    "    ['Extra Trees', et_score, et_mae, et_scores.mean(), et_scores.std()],\n",
    "    ['Hist Gradient Boosting', hgb_score, hgb_mae, hgb_scores.mean(), hgb_scores.std()],\n",
    "    ['SVR', svr_score, svr_mae, svr_scores.mean(), svr_scores.std()],\n",
    "    ['KNN', knn_score, knn_mae, knn_scores.mean(), knn_scores.std()],\n",
    "    ['XGBoost', xgb_score, xgb_mae, xgb_scores.mean(), xgb_scores.std()],\n",
    "    ['AdaBoost', ab_score, ab_mae, ab_scores.mean(), ab_scores.std()],\n",
    "    ['LGBMRegressor', lgbm_score, lgbm_mae, lgbm_scores.mean(), lgbm_scores.std()],\n",
    "    ['Bagging', bag_score, bag_mae, bag_scores.mean(), bag_scores.std()],\n",
    "    ['Hard Voting', voting_score, voting_mae, voting_scores.mean(), voting_scores.std()],\n",
    "    ['Stacking', stacking_score, stacking_mae, stacking_scores.mean(), stacking_scores.std()],\n",
    "    ['Bagged KNN', bag_knn_score, bag_knn_mae, bag_knn_scores.mean(), bag_knn_scores.std()],\n",
    "    ['Bagged DT', bag_dt_score, bag_dt_mae, bag_dt_scores.mean(), bag_dt_scores.std()],\n",
    "]\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[1], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[1])\n",
    "worst_model = min(results, key=lambda x: x[1])\n",
    "\n",
    "table = Table(title=\"Embedded Method Comparison\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"R2 score\")\n",
    "table.add_column(\"Mean Absolute Error\")\n",
    "table.add_column(\"K-Fold mean\")\n",
    "table.add_column(\"K-Fold std\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, r2, mae, kmean, kstd = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{r2:.2f}[/bold green]\",\n",
    "            f\"[bold green]{mae:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{r2:.2f}[/bold red]\",\n",
    "            f\"[bold red]{mae:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(algo, f\"{r2:.2f}\", f\"{mae:.2f}\", f\"{kmean:.2f}\", f\"{kstd:.2f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7000add5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Embedded Method Comparison                              </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm              </span><span style=\"font-weight: bold\"> R2 score </span><span style=\"font-weight: bold\"> Mean Absolute Error </span><span style=\"font-weight: bold\"> K-Fold mean </span><span style=\"font-weight: bold\"> K-Fold std </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Linear Regression</span>       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       \n",
       "\n",
       " Decision Tree           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Ridge                   1.00      104.34               1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      81.04                1.00         0.00       \n",
       "\n",
       " Lasso                   1.00      146.54               1.00         0.00       \n",
       "\n",
       " SVR                     0.99      342.72               0.98         0.00       \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ElasticNet</span>              <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.72</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2694.71</span>              <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.72</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Embedded Method Comparison                              \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mR2 score\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mMean Absolute Error\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mLinear Regression\u001b[0m       \u001b[1;32m1.00\u001b[0m      \u001b[1;32m0.00\u001b[0m                 \u001b[1;32m1.00\u001b[0m         \u001b[1;32m0.00\u001b[0m       \n",
       "\n",
       " Decision Tree           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Ridge                   1.00      104.34               1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      81.04                1.00         0.00       \n",
       "\n",
       " Lasso                   1.00      146.54               1.00         0.00       \n",
       "\n",
       " SVR                     0.99      342.72               0.98         0.00       \n",
       "\n",
       " \u001b[1;31mElasticNet\u001b[0m              \u001b[1;31m0.72\u001b[0m      \u001b[1;31m2694.71\u001b[0m              \u001b[1;31m0.72\u001b[0m         \u001b[1;31m0.00\u001b[0m       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/feature_selection_compare.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5472353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
